#!/usr/bin/env python

from collections import OrderedDict
from uuid import getnode as get_mac
import argparse
import glob
import json
import os
import string
import subprocess
import sys
import urllib2
import warnings
import zipfile
import distutils.dir_util

root_path = os.path.abspath('.')
default_json_file_path = root_path + '/cppdock.json'
default_dep_path = "https://github.com/"
cluster_build_nodes = None

def remove_none(obj):
  if isinstance(obj, (list, tuple, set)):
    return type(obj)(remove_none(x) for x in obj if x is not None)
  elif isinstance(obj, dict):
    return type(obj)((remove_none(k), remove_none(v))
      for k, v in obj.items() if k is not None)
  else:
    return obj

def get_commands():
    return {
        'init':                 (command_init,              parse_args_init),
        'build':                (command_build,             parse_args_build),
        'build_nginx_proxy':    (command_build_nginx_proxy, parse_args_build_nginx_proxy),
        'dev':                  (command_dev,               parse_args_dev),
        'dev_cluster':          (command_dev_cluster,       parse_args_dev_cluster),
        'dev_registry':         (command_dev_registry,      parse_args_dev_registry),
        'install_dep':          (command_install_dep,       parse_args_install_dep),
        'install_src':          (command_install_src,       parse_args_install_src),
        'init_dev_service':     (command_init_dev_service,  parse_args_init_dev_service),
        '_complete':            (command_complete,          lambda _ : os.sys.argv[2]),
    }

def get_subcommands():
    return filter(lambda x: not x.startswith('_'), get_commands().keys())

def parse_args_command(args):
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument('command', choices = get_commands().keys())
    return parser.parse_args(args)

def parse_args_init(args):
    parser = argparse.ArgumentParser(prog='cppdock init')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')
    parser.add_argument('--print-config',
                                        dest      = 'print_config',
                                        action    = 'store_true',
                                        help      = 'Print consolidated config')

    return parser.parse_args(args)

def parse_args_build(args):
    parser = argparse.ArgumentParser(prog='cppdock build')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')

    parser.add_argument('--print-only', dest      = 'print_only',
                                        action    = 'store_true',
                                        help      = 'Print Dockerfile without building')
    parser.add_argument('--domain',     dest      = 'domain',
                                        default   = '',
                                        help      = 'Domain for web proxy')
    parser.add_argument('--tag',        dest      = 'tag',
                                        default   = '',
                                        help      = 'Tag name if no domain')
    parser.add_argument('platform',     default   = None,
                                        nargs     = '?',
                                        help      = 'Name of target platform')

    return parser.parse_args(args)

def parse_args_build_nginx_proxy(args):
    parser = argparse.ArgumentParser(prog='cppdock build_nginx_proxy')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')

    parser.add_argument('--print-only', dest      = 'print_only',
                                        action    = 'store_true',
                                        help      = 'Print Dockerfile without building')
    parser.add_argument('domain',       default   = None,
                                        help      = 'Domain name of proxy server')

    return parser.parse_args(args)

def parse_args_dev(args):
    parser = argparse.ArgumentParser(prog='cppdock dev')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')
    parser.add_argument('-p', '--port', dest      = 'port',
                                        help      = 'Specify port to `docker run`')

    group  = parser.add_argument_group('required arguments')
    group.add_argument('platform',      help      = 'Name of target platform')

    return parser.parse_args(args)

def parse_args_dev_cluster(args):
    parser = argparse.ArgumentParser(prog='cppdock dev_cluster')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')
    parser.add_argument('--print-only', dest      = 'print_only',
                                        action    = 'store_true',
                                        help      = 'Print Compose file without deploying')
    parser.add_argument('--stop',       dest      = 'stop',
                                        action    = 'store_true',
                                        help      = 'Stops the dev cluster services')
    parser.add_argument('--gen-certs',  dest      = 'gen_certs',
                                        action    = 'store_true',
                                        help      = 'Generates self signed certs for the domain')
    parser.add_argument('domain',       default   = None,
                                        help      = 'Domain name or tag if no proxy')

    return parser.parse_args(args)

def parse_args_dev_registry(args):
    parser = argparse.ArgumentParser(prog='cppdock dev_registry')
    parser.add_argument('--stop',       dest      = 'stop',
                                        action    = 'store_true',
                                        help      = 'Stops the dev registry')
    return parser.parse_args(args)

def parse_args_install_dep(args):
    parser = argparse.ArgumentParser(prog='cppdock install_dep')
    group  = parser.add_argument_group('required arguments')
    group.add_argument('platform',      help      = 'Token for target platform')
    group.add_argument('branch',        help      = 'SHA1 of git revision')
    group.add_argument('repo',          help      = 'Name of git repo (e.g. boostorg/hana)')
    return parser.parse_args(args)

def parse_args_install_src(args):
    parser = argparse.ArgumentParser(prog='cppdock install_src')
    group  = parser.add_argument_group('required arguments')
    group.add_argument('branch',        help      = 'SHA1 of git revision')
    group.add_argument('repo',          help      = 'Name of git repo (e.g. llvm-mirror/clang)')
    group.add_argument('dest_dir',      help      = 'Path to copy source files to')
    group  = parser.add_argument_group('optional arguments')
    group.add_argument('src_dir',       nargs='?',
                                        default='.',
                                        help      = 'Path of subdirectory to copy from repo')
    return parser.parse_args(args)

def parse_args_init_dev_service(args):
    parser = argparse.ArgumentParser(prog='cppdock init_dev_service')
    parser.add_argument('--shell-only', dest      = 'shell_only',
                                        action    = 'store_true',
                                        help      = 'Run only as an interactive shell')
    group  = parser.add_argument_group('required arguments')
    group.add_argument('build_type',    help      = 'Build type')
    group.add_argument('target',        help      = 'Build target name')

    return parser.parse_args(args)

def dispatch(command_args, cli_args):
    command = parse_args_command(command_args).command
    commands = get_commands()
    if command in commands:
        args = commands[command][1](cli_args)
        commands[command][0](args)

def command_complete(args_str):
    subcommands = get_subcommands()

    args = args_str.split();
    has_trailing_space = args_str.endswith(' ')
    # length includes a trailing empty string if there is a trailing space
    if args_str.endswith(' '):
        args.append('')
    # filter out completed options
    args = filter(lambda x: not x.startswith('-'), args[:-1]) + [args[-1]]
    length = len(args)

    if length < 2:
        # should not be possible from the completion script
        return

    candidates = []
    completion = ''

    if length == 2:
        # complete subcommand
        completion = args[1]
        candidates = subcommands
    elif length == 3:
        # complete platform
        subcommand = args[1]
        if subcommand in ['build', 'dev']:
            completion = args[2]
            config = load_config_file(default_json_file_path)
            candidates = get_all_platforms(config)

    candidates = filter(lambda x: x.startswith(completion), candidates)
    print ' '.join(candidates)

def command_init(args):
    config = load_config_file_with_args(args)
    if args.print_config:
        print json.dumps(config, indent=2, separators=(',', ': '))

def command_build(args):
    config = load_config_file_with_args(args)

    platforms = [args.platform] if args.platform else get_all_platforms(config)
    for platform in platforms:
        make_build_image(config, platform, args.print_only)

    if args.domain:
        make_nginx_proxy_image(config, args.domain, args.print_only)

def make_build_image(config, platform, print_only):
    if is_platform_service(config, platform):
        return

    make_base_image(config, platform, print_only)
    make_dep_image(config, platform, print_only)

def command_build_nginx_proxy(args):
    config = load_config_file_with_args(args)
    make_nginx_proxy_image(config, args.domain, args.print_only)

def make_nginx_proxy_image(config, domain, print_only):
    tag_name = get_image_tag_name(config, 'nginx_proxy', 'service')
    conf_data = get_nginx_proxy_conf(config, domain)
    dockerfile = """
    FROM nginx:alpine
        RUN echo $'{conf_data}' > /etc/nginx/nginx.conf
    """.format(conf_data = conf_data.replace('\n', '\\n\\\n'))

    make_docker_image(tag_name, dockerfile, print_only)

def command_dev(args):
    config = load_config_file_with_args(args)
    platform_name = args.platform
    ports_arg = ' -p=' + args.port if hasattr(args, 'port') and args.port else ''
    make_dev_image(config, platform_name)
    run_dev_image(config, platform_name, ports_arg)

def command_dev_cluster(args):
    domain      = args.domain
    config      = load_config_file_with_args(args)
    stack_name  = get_stack_name(config)

    if args.gen_certs:
        import_ssl_certificate(*generate_self_signed_ssl_certificate(domain))

    if (args.stop):
        subprocess.check_output(['docker', 'stack', 'rm', stack_name])
    elif (args.print_only):
        print get_dev_compose_file(config, domain)
    else:
        nodes = cluster_get_build_node_ids()
        if is_cluster_mode():
            print "Running in CLUSTER mode...\n"
            print "Nodes found: " + ', '.join(nodes)
        else:
            print "Running in LOCAL mode..."

        cluster_sync_source(config, nodes)
        # builds, tags, and pushes the dev images which should be to the dev_registry
        for platform_name in get_all_platforms(config):
            # TODO push service images too
            if get_platform_type(config, platform_name).endswith('service'):
                continue
            tag_name = make_dev_image(config, platform_name)
            push_docker_image(config, tag_name)

            nginx_proxy_tag_name = get_image_tag_name(config, 'nginx_proxy', 'service')
            push_docker_image(config, nginx_proxy_tag_name)

        run_stack_deploy(config, stack_name, domain)

        print "Finished deploying {stack_name}\n".format(stack_name = stack_name)
        dev_service_name = subprocess.check_output(['docker', 'ps', '-q', '-f name=servicename'])

def command_dev_registry(args):
    # docker service create --name test_registry --publish published=5000,target=5000 registry
    env_val = os.getenv('CPPDOCK_DEV_REGISTRY')
    registry_name = 'cppdock-dev-registry'
    if env_val:
        raise 'ERROR: CPPDOCK_DEV_REGISTRY is set ({})'.format(env_val)

    if args.stop:
        subprocess.check_output(['docker', 'service', 'rm', registry_name])
    else:
        subprocess.check_output(['docker', 'service', 'create', '--name',
                                 registry_name, '--publish', '5000:5000', 'registry'])


def command_install_dep(args):
    # This called inside a container so config is not allowed
    # Even if we add a merged config file to the container we
    # do not want it available at this stage or things will
    # be rebuilt for every config change
    check_sysroot()
    url = make_archive_url(args.repo, args.branch)
    input = urllib2.urlopen(url)
    output = open('dep.zip', 'wr')
    buf = ''
    while True:
        buf = input.read(800)
        output.write(buf)
        output.flush()
        if (len(buf) == 0):
            break
    output.close()

    assert os.path.isfile('dep.zip')
    assert zipfile.is_zipfile('dep.zip')

    zipfile.ZipFile('dep.zip', 'r').extractall()
    recipe = find_recipe(args.platform, args.repo)
    cwd_dir = [x for x in os.listdir('.') if x.endswith(args.branch)][0]
    p = subprocess.Popen([recipe], cwd = cwd_dir)
    p.communicate('')
    if p.returncode != 0:
        print """

    FAILURE: Recipe returned error code {0}.
        """.format(p.returncode)
        sys.exit(1)

def command_install_src(args):
    url = make_archive_url(args.repo, args.branch)
    input = urllib2.urlopen(url)
    output = open('dep.zip', 'wr')
    buf = ''
    while True:
        buf = input.read(800)
        output.write(buf)
        output.flush()
        if (len(buf) == 0):
            break
    output.close()

    assert os.path.isfile('dep.zip')
    assert zipfile.is_zipfile('dep.zip')

    zipfile.ZipFile('dep.zip', 'r').extractall()
    base_src_dir = [x for x in os.listdir('.') if x.endswith(args.branch)][0]
    src_dir = os.path.join(base_src_dir, args.src_dir);
    distutils.dir_util.copy_tree(src_dir, args.dest_dir)
    distutils.dir_util.remove_tree(base_src_dir);

def command_init_dev_service(args):
    # command run in container so config is not allowed
    run_dev_service_target(args.build_type, args.target, args.shell_only)

def install_deps_platform(platform, items):
    for i in range(len(items)):
        repo, branch = items[i]
        branch = branch[0:40]
        install_dep(i, platform, repo, branch)

def load_config_file_with_args(args):
    json_file_path = args.json_file_path
    return load_config_file(json_file_path)

def load_config_file(json_file_path):
    config_paths = []
    config_paths.insert(0, json_file_path)
    config = load_config_file_json(json_file_path)

    # we do not attempt to lock deps in the base_config
    config = lock_config_file_deps(config, json_file_path)
    config = normalize_config_paths(config)
    config = merge_base_config(config, config_paths)
    config = process_config_replace_deps(config)

    source_path = config_paths[0]
    if not os.path.isdir(source_path):
        source_path = os.path.dirname(source_path)

    cppdock = config['cppdock']
    source_path = cppdock.get("source_path", source_path)
    cppdock["source_path"] = source_path
    cppdock["cppdock_image"] = cppdock.get("cppdock_image",
                                           "ricejasonf/cppdock")
    return config

def load_config_file_json(json_file_path):
    if os.path.isdir(json_file_path):
        json_file_path = os.path.join(json_file_path, 'cppdock.json')

    if not os.path.isfile(json_file_path):
        raise ValueError('Config file not found: ' + json_file_path)
    stream = file(json_file_path, 'r')
    config = json.load(stream, object_pairs_hook=OrderedDict)

    # validate
    assert "cppdock" in config,             "Config must have cppdock section"
    assert "name" in config["cppdock"],     "Config must have 'name' in cppdock section"

    return config

def normalize_config_paths(config):
    # this should happen after writing revision locks
    cppdock = config["cppdock"]
    if "base_config" in cppdock:
        cppdock["base_config"] = os.path.abspath(cppdock["base_config"])
    if "source_path" in cppdock:
        cppdock["source_path"] = os.path.abspath(cppdock["source_path"])
    return config

def write_config_file(config, json_file_path):
    try:
        with open(json_file_path, 'wb') as fh:
            json.dump(config, fh, indent=2, separators=(',', ': '))
            fh.write('\n')
    except:
        warnings.warn('WARNING: Unable to write to json file')

def merge_base_config(config, config_paths):
    if "base_config" not in config["cppdock"]:
        return config
    base_config_path = os.path.abspath(config["cppdock"]["base_config"])
    del config["cppdock"]["base_config"]
    config_paths.insert(0, base_config_path)
    base_config = load_config_file_json(base_config_path)

    # merge cppdock section
    cppdock_section = {}
    cppdock_section.update(base_config["cppdock"])
    cppdock_section.update(config["cppdock"])
    config["cppdock"] = cppdock_section
    platforms = {}
    platforms.update(base_config.get("platforms", {}))

    for platform_name in config.get("platforms", {}):
        platforms[platform_name] = merge_platform_config(platform_name, base_config, config)

    config["platforms"] = platforms
    return config

def merge_platform_config(platform_name, base_config, config):
    result = {}
    platform = config["platforms"].get(platform_name, {})
    base_platform = base_config.get("platforms", {}).get(platform_name, {})

    result.update(base_platform)
    result.update(platform)
    result["deps"] = []
    result["deps"] += base_platform.get("deps", [])
    result["deps"] += platform.get("deps", [])
    return result

def process_config_replace_deps(config):
    # TODO read "replace_deps" to replace all deps with a
    #      specified name or a dep with a specific id
    #      not sure if this is desirable yet
    return config

def lock_config_file_deps(config, json_file_path):
    if 'platforms' not in config:
        return config
    for platform, settings in config['platforms'].items():
        check_base_image_conflict(settings)

        for stage in settings.get('deps', []):
            if not isinstance(stage, list):
                raise "Each stage in 'deps' must be an array."
            for item in stage:
                item = lock_item_json(item)

    write_config_file(config, json_file_path)
    return config

def lock_item_json(item):
    current_sha = get_current_sha_with_comment(item)
    item.update(revision = current_sha)

    return item

def get_current_sha_with_comment(item):
    repo = item['name']
    tag = item.get('tag')
    source = item.get('source')

    if 'revision' in item:
        return item['revision']
    else:
        if source == 'docker':
            tag = tag or 'latest'
            repo_tag = repo + ":" + tag

            try:
                # pull the image from the registry to get the RepoDigest
                subprocess.check_output(['docker', 'pull', repo_tag])
            except:
                raise RuntimeError("Unable to pull docker image from registry: " + repo_tag )

            try:
                inspect_result =  subprocess.check_output(['docker', 'inspect', repo_tag])
                digest = json.loads(inspect_result)[0]['RepoDigests'][0].split('@')[1]
            except:
                raise RuntimeError("Unable to get SHA from docker image")

            return digest
        else:
            tag = normalize_branch_name(tag)
            lines = subprocess.check_output(['git', 'ls-remote', normalize_repo_name(repo)]).splitlines()
            for line in lines:
                if line.endswith(tag):
                    return line[0:40]
            raise RuntimeError("Unable to get SHA from remote git repository")

def normalize_repo_name(name):
    return default_dep_path + name;

def normalize_branch_name(name):
    if name == None or len(name) == 0 or name == 'HEAD':
        return 'HEAD'
    elif name.startswith('refs/'):
        return name
    else:
        return "refs/heads/" + name

def is_sha(name):
    return len(name) == 40

def check_base_image_conflict(settings):
    if 'base_image' in settings and 'base_image_dockerfile' in settings:
        raise StandardError("Conflicting base image settings detected")

def get_max_key_length(config):
    length = 0
    for section in config.sections():
        for item in config.items(section):
            len_ = len(item[0])
            if (len_ > length):
                length = len_
    return length

def make_archive_url(repo, branch):
    return 'https://github.com/{0}/archive/{1}.zip'.format(repo, branch)

def get_base_image_dockerfile(config, platform_name):
    return config['platforms'][platform_name].get('base_image_dockerfile')

def get_user_base_tag_name(config, platform_name):
    if get_base_image_dockerfile(config, platform_name):
        return get_image_tag_name(config, platform_name, 'user_base')

    tag_name =  config['platforms'][platform_name].get('base_image')
    if tag_name:
        return tag_name

    platform_type = get_platform_type(config, platform_name)
    return get_config_option(config, 'platform_' + platform_type)

def get_base_image_dockerfile_data(config, platform_name):
    tag_name = get_user_base_tag_name(config, platform_name)
    cppdock_tag_name = config['cppdock']['cppdock_image']

    return """
FROM {tag_name}
    # installs cppdock in base image
    COPY --from={cppdock_tag_name} /opt/install/ /usr/local
    """.format(tag_name = tag_name,
               cppdock_tag_name = cppdock_tag_name)

def make_base_image(config, platform_name, print_only):
    make_base_image_dockerfile(config, platform_name, print_only)
    dockerfile = get_base_image_dockerfile_data(config, platform_name)
    tag_name = get_base_tag_name(config, platform_name)
    make_docker_image(tag_name, dockerfile, print_only)

def make_base_image_dockerfile(config, platform_name, print_only):
    base_image_dockerfile = get_base_image_dockerfile(config, platform_name)

    if base_image_dockerfile:
        dockerfile = "./{0}".format(base_image_dockerfile)
        tag_name = get_user_base_tag_name(config, platform_name)

        with open(dockerfile, 'r') as file:
            dockerfile_data = file.read()
        make_docker_image(tag_name, dockerfile_data, print_only)

def find_recipe(platform, repo):
    repo = repo.replace('/', '-')
    local_recipe_path = '/opt/cppdock_recipes'
    repo_with_platform = repo + '-' + platform
    builtin_recipe_path = os.path.expanduser('~') + '/.cppdock_recipes'
    xs = [
        local_recipe_path   + '/' + repo_with_platform,
        local_recipe_path   + '/' + repo,
        local_recipe_path   + '/default',
        builtin_recipe_path + '/' + repo_with_platform,
        builtin_recipe_path + '/' + repo,
        builtin_recipe_path + '/default',
    ]
    for x in xs:
        print "Trying Recipe Location: {0}".format(x)
        if os.path.isfile(x):
            return x
    raise ValueError('Unable to find cppdock recipe: ' + repo_with_platform)

def check_sysroot():
    if not os.path.isdir('/opt/sysroot/'):
        raise ValueError('Sysroot directory is missing: /opt/sysroot/')

def get_arg(args, i):
    next(iter(args[i:i+1]), None)

def get_config_option(config, name):
    defaults = {
        'project': None,
        'platform_linux_x64':     'ricejasonf/cppdock:linux_x64',
        'platform_emscripten':    'ricejasonf/cppdock:emscripten',
        'platform_tvossimulator': 'ricejasonf/cppdock:tvossimulator',
    }
    value = None

    if name in defaults:
        value = defaults[name]
    if value == None:
        raise ValueError('Config option has no default for "{0}"'.format(name))
    return value

def make_dep_image(config, platform_name, print_only):
    dockerfile = make_deps_dockerfile(config, platform_name)
    tag_name = get_build_tag_name(config, platform_name)
    make_docker_image(tag_name, dockerfile, print_only)

def make_docker_image(tag_name, dockerfile_data, print_only = False):
    if print_only:
        print '###### Docker Image: ' + tag_name + ' ######'
        print dockerfile_data
        return
    p = subprocess.Popen(['docker', 'build', '--tag=' + tag_name, '--file=-', '.'], stdin=subprocess.PIPE)
    out, err = p.communicate(dockerfile_data)
    if p.returncode == 0:
        print "Finished building {0}.".format(tag_name)
    else:
        print """

    FAILURE: Build of {0} FAILED.

        """.format(tag_name)
        sys.exit(1)

    return (p.returncode, out, err)

def push_docker_image(config, tag_name):
    # tags with registry and pushes
    registry = get_registry(config)
    rtag_name = registry + '/' + tag_name
    subprocess.check_output(['docker', 'tag', tag_name, rtag_name])
    subprocess.check_output(['docker', 'push', rtag_name])
    print "Finished pushing {0}".format(rtag_name)
    return rtag_name

def get_platform_type(config, platform_name):
    try:
        return config['platforms'][platform_name]['type']
    except:
        raise ValueError('Platform name `{0}` not found.'.format(platform_name))

# is_platform_service(config, platform_name)
# is_platform_service(platform_type)
def is_platform_service(*argv):
    if len(argv) == 1:
        [platform_type] = argv
    elif len(argv) == 2:
        [config, platform_name] = argv
        platform_type = get_platform_type(config, platform_name)
    return platform_type.endswith('service')

def get_project_name(config):
    return config['cppdock']['name']

def make_deps_dockerfile(config, platform_name):
    platform_type = get_platform_type(config, platform_name)
    # move this to be used when building the base image
    #base_image = get_base_image(config, platform_name) or get_config_option(config, 'platform_' + platform_type)
    base_image = get_base_tag_name(config, platform_name)
    deps = get_deps_from_config(config, platform_name)
    deps_stages = ''
    deps_imports = ''

    for dep in deps:
        deps_stages += string.join([make_deps_stage(base_image, platform_name, dep)])
        deps_imports += make_deps_import(platform_name, dep[-1][2])

    return """
{0}
FROM {1}
{2}
""".format(deps_stages, base_image, deps_imports)

# deprecated
def make_deps_dockerfile_stageless(config, platform):
    base_image  = get_config_option(config, 'platform_' + platform)
    deps = get_deps_from_config(config, platform)
    deps_stages = string.join([make_install_dep(x, y, z, zz) for x, y, z, zz in deps])

    return """
FROM {0} as build_all
{1}

FROM {2}

    COPY --from=build_all /opt/install /opt/sysroot
""".format(base_image, deps_stages, base_image)

def make_deps_stage(base_image, platform_name, deps):
    copy_recipes_term = ""
    build_image = base_image

    # only one dep that is source: docker does not use make_install_dep

    docker_image_digest = get_stage_docker_image_digest(deps)
    if docker_image_digest is not None:
    	build_image  = docker_image_digest
        install_deps = ''
    else:
        install_deps = string.join([make_install_dep(x, y, z, zz) for x, y, z, zz in deps])

    revision = deps[-1][2].replace('/', '_')

    return """
FROM {0} as build_{2}_{1}
    {4}
    WORKDIR /usr/local/src
    {3}
""".format(build_image, platform_name, revision, install_deps, copy_recipes_term)

def get_stage_docker_image_digest(stage_deps):
    if len(stage_deps) == 1:
        for platform, revision, repo, source in stage_deps:
            if source == 'docker':
                # repo@revision
                return '{0}@{1}'.format(repo, revision)
    return None


def make_install_dep(platform, revision, repo, source=''):
    if source == 'docker':
        return """
    COPY --from={0}@{1} /opt/install /opt/sysroot
    COPY --from={0}@{1} /opt/install /opt/install""".format(repo, revision)

    if not is_sha(revision):
        return ''
    repo_recipe_prefix = repo.replace('/', '-')
    copy_local_recipe = ''
    if os.path.isfile(os.path.join(root_path, 'cppdock_recipes', repo_recipe_prefix + '-' + platform)):
        copy_local_recipe = """
    COPY cppdock_recipes/{0}-{1} /opt/cppdock_recipes/""".format(repo_recipe_prefix, platform)
    elif os.path.isfile(os.path.join(root_path, 'cppdock_recipes', repo_recipe_prefix)):
        copy_local_recipe = """
    COPY cppdock_recipes/{0} /opt/cppdock_recipes/""".format(repo_recipe_prefix)

    return """{3}
    RUN cppdock install_dep {0} {1} {2}""".format(platform, revision, repo, copy_local_recipe)

def make_deps_import(platform, revision):
    return """
    COPY --from=build_{1}_{0} /opt/install/ /opt/sysroot""".format(platform, revision.replace('/', '_'))

def get_deps_from_config(config, platform_name):
    platform = config['platforms'][platform_name]

    return [get_deps_stage_items(stage, platform['type']) for stage in platform['deps']]

    raise ValueError('Platform is not specified in cppdock json file')

def get_deps_stage_items(deps_stage, platform_type):
    return [(platform_type, item['revision'],  item['name'], item.get('source')) for item in deps_stage]

def make_dev_image(config, platform_name):
    build_tag_name = get_build_tag_name(config, platform_name)
    dev_tag_name = get_dev_tag_name(config, platform_name)
    # TODO get build type from config['cppdock']['build_type']
    build_type = 'debug'
    target = get_dev_service_target(config, platform_name)

    dockerfile = """
FROM {build_tag_name}
    WORKDIR /opt/build
    CMD cppdock init_dev_service --shell-only {build_type} {target}
""".format(build_tag_name = build_tag_name,
           build_type = build_type,
           target = target)

    p = subprocess.Popen(['docker', 'build', '--tag=' + dev_tag_name, '--file=-', '.'], stdin=subprocess.PIPE)
    out, err = p.communicate(dockerfile)
    if p.returncode == 0:
        print "Finished building {0}.".format(dev_tag_name)
    else:
        print """

    FAILURE: Build of {0} FAILED.
        """.format(dev_tag_name)
        sys.exit(1)

    return dev_tag_name

def run_dev_image(config, platform_name, ports_arg):
    tag_name = get_dev_tag_name(config, platform_name)
    pwd = config['cppdock']['source_path']
    os.system('docker run --rm -it ' + ports_arg + ' --mount type=bind,source=' + pwd + ',target=/opt/src,readonly ' + tag_name)

def run_stack_deploy(config, stack_name, domain):
    # docker stack deploy is already idempotent
    compose_file_data = get_dev_compose_file(config, domain)
    p = subprocess.Popen(['docker', 'stack', 'deploy', '--prune', '--compose-file', '-', stack_name], stdin=subprocess.PIPE)
    p.communicate(input=compose_file_data)
    if p.returncode != 0:
        print """

    FAILURE: `docker stack deploy` for `{stack_name}` error code {error_code}.
        """.format(stack_name=stack_name, error_code=p.returncode)
        sys.exit(1)

def run_stack_service_terminal(config, stack_name, platform_name):
    service_name = get_service_name(config, platform_name)
    print """
    TODO shell into {service_name} somehow
    """
    # container_id = # get_container_id(stack_name, platform_name)
    # TODO finish
    #os.system("docker exec -it {0} bash".format(container_id)

def get_dev_compose_file(config, domain):
    platform_names = get_all_platforms(config)
    services = [get_dev_compose_file_service(config, platform_name, domain) for platform_name in platform_names]
    secrets = config.get('secrets', {})
    secrets.update(get_secret_files());


    # TODO break out nginx_proxy stuff
    #      to get_dev_compose_file_service
    crt = '{}.crt'.format(domain)
    key = '{}.key'.format(domain)
    secrets.update({
        crt: { 'external': True },
        key: { 'external': True }
    })
    services.append(get_nginx_proxy_service(config, domain))

    services_dict = {}
    for x in services:
        services_dict.update(x)

    result = {
        'version': '3.1',
        'services': services_dict,
        'volumes': {
            'web_root': None
        },
        'secrets': secrets
    }

    return json.dumps(remove_none(result), indent=2, separators=(',', ': '))

def get_dev_compose_file_service(config, platform_name, domain):
    platform = get_platform_config(config, platform_name)
    service_name = get_service_name(config, platform_name)
    image_tag = get_dev_tag_name(config, platform_name)
    target = get_dev_service_target(config, platform_name)
    build_type = 'debug'

    command = "cppdock init_dev_service {build_type} {target}".format(
            build_type = build_type,
            target = target)

    web_root = platform.get('web_root')
    if web_root:
        web_root = "web_root:{0}".format(web_root)

    pwd = config['cppdock']['source_path']
    remote_pwd = cluster_get_sync_path(pwd, 'src')
    src_mount_path = remote_pwd if is_cluster_mode() else pwd
    src_mount = '{src_mount_path}:/opt/src:ro'.format(src_mount_path = src_mount_path)
    environment = { 'DOMAIN': domain }
    environment.update(platform.get('environment', {}))
    secrets = platform.get('secrets', [])

    return {
        service_name: {
            'image': image_tag,
            'command': command,
            'environment': environment,
            'secrets': secrets,
            'volumes': [
                src_mount,
                '/opt/build',
                web_root
            ]
        }
    }

#def get_compose_file_external_service(config, platform_name)
#    platform = get_platform(config, platform_name)
#    assert is_platform_service(platform.type), 'External platform must be type "service"'
    # TODO copy key/value all except "tag" and "revision"
    # TODO call this function when building dev_compose_file

def get_build_tag_name(config, platform_name):
    return get_image_tag_name(config, platform_name, "build")

def get_dev_tag_name(config, platform_name):
    return get_image_tag_name(config, platform_name, "dev")

def get_base_tag_name(config, platform_name):
    return get_image_tag_name(config, platform_name, 'base')

def get_image_tag_name(config, platform_name, image_type):
    assert(image_type in ['build', 'dev', 'release', 'service', 'base', 'user_base'])
    return "{project}-{platform}{image_type}".format(
        project     = get_project_name(config),
        platform    = platform_name,
        image_type  = '-' + image_type if image_type != 'release' else '',
    )

def get_stack_name(config):
    return get_project_name(config)

def default_build_type(config, platform_name):
    if config['platforms'][platform_name]['type'] == 'emscripten' :
        return 'Release'
    else:
        return 'Debug'

def get_all_platforms(config):
    return config['platforms'].keys()

def get_platform_config(config, platform_name):
    return config['platforms'][platform_name]

def get_dev_service_target(config, platform_name):
    platform = get_platform_config(config, platform_name)
    service_target = platform.get('service_target')
    return platform.get('target') or service_target

def run_dev_service_cmake(build_type):
    if not os.path.isfile('/opt/build/CMakeCache.txt'):
        p = subprocess.Popen(['cmake', '-DCMAKE_BUILD_TYPE=' + build_type,
                              "-DCMAKE_TOOLCHAIN_FILE='/opt/toolchain.cmake'",
                              '/opt/src'],
                             cwd = '/opt/build')
    else:
        p = subprocess.Popen(['cmake', '-DCMAKE_BUILD_TYPE=' + build_type,
                              '.'],
                             cwd = '/opt/build')
    p.communicate('')
    return p.returncode == 0

def run_dev_service_target(build_type, target, shell_only = False):
    # This should run in a docker container
    # Initializes CMake if needed

    if shell_only:
        run_dev_service_cmake(build_type)
        p = subprocess.call('/bin/bash')
        return

    p = None
    command = "cmake --build . --target " + target
    if run_dev_service_cmake(build_type) and target:
        p = subprocess.Popen(['cmake', '--build', '.', '--target', target],
                             cwd = '/opt/build')
        p.communicate('')

    # Make the service target and just hang out
    # Things will show up in the logs and the user can still shell in to inspect things

    if p and p.returncode != 0:
        print "Service exited with error {}.".format(p.returncode)

    # hang out so the container stays alive

    p = subprocess.Popen(['tail', '-f', '/dev/null'])
    p.communicate('')

def cluster_get_worker_token():
    try:
        return subprocess.check_output(['docker', 'swarm', 'join-token', '-q', 'worker'])
    except:
        raise RuntimeError("Unable to get cluster worker token")

def cluster_get_build_node_ids():
    global cluster_build_nodes
    if cluster_build_nodes is not None:
        return cluster_build_nodes
    try:
        result = subprocess.check_output(['docker-machine', 'ls',
                                          '-q', '--filter', 'label=cppdock_build=1',
                                          '--filter', 'state=Running'])
        cluster_build_nodes = result.splitlines()
        return cluster_build_nodes
    except:
        raise RuntimeError("Unable to get cluster ids for cppdock_build nodes")

def cluster_get_sync_path(path, kind):
    assert kind in ['src']
    return "/home/ubuntu/sync{path}/{kind}".format(path = path,
                                                   kind = kind)

# rsyncs everything in pwd to nodes in cluster
def cluster_sync_source(config, nodes):
    nodes = cluster_get_build_node_ids()
    pwd = config['cppdock']['source_path']
    for node in nodes:
        path = cluster_get_sync_path(pwd, 'src')
        p = subprocess.Popen(['docker-machine', 'ssh', node,
                              'mkdir', '-p' ' {path}'.format(path=path)])
        p.communicate('')
        if p.returncode != 0:
            raise RuntimeError("Unable to make the sync directory on {node}".format(node = node))
        node_path = "ubuntu@{node}:{path}".format(node = node,
                                           path = path)
        p = subprocess.Popen(['docker-machine', 'scp', '-r', '-d', '.', node_path])
        p.communicate('')

        if p.returncode != 0:
            raise RuntimeError("Failed to sync source files to {node}".format(node = node))

def get_nginx_proxy_conf(config, domain):
    locations           = ''
    upstreams           = ''
    # TODO get domain from config
    redirect_servers    = ''

    if domain.startswith('www.'):
        non_www_domain = domain[4:]
        redirect_servers += """
    server {{
        # redirect to https
	server_name {domain} {non_www_domain};
	return 301 https://{domain}$request_uri;
    }}
    server {{
        # redirect to www.
        listen 443 ssl;
        server_name {non_www_domain};
        ssl_certificate /run/secrets/{domain}.crt;
        ssl_certificate_key /run/secrets/{domain}.key;
	return 301 https://{domain}$request_uri;
    }}""".format(domain         = domain,
                 non_www_domain = non_www_domain)
    else:
        redirect_servers += """
    server {{
        # redirect to https
	server_name {domain};
	return 301 https://{domain}$request_uri;
    }}""".format(domain = domain)

    for platform_name in get_all_platforms(config):
        if get_platform_type(config, platform_name) == 'service':
            continue;
        locations += get_nginx_proxy_conf_locations(config, platform_name)
        upstreams += get_nginx_proxy_conf_upstreams(config, platform_name)
    return """
user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {{
  worker_connections  1024;
}}

http {{
    include       /etc/nginx/mime.types;
    types {{
        application/wasm wasm;
        font/woff2 woff2;
    }}
    default_type  application/octet-stream;
    resolver 127.0.0.11;

    log_format  main  \\'$remote_addr - $remote_user [$time_local] "$request" \\'
		      \\'$status $body_bytes_sent "$http_referer" \\'
		      \\'"$http_user_agent" "$http_x_forwarded_for"\\';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    {upstreams}

    {redirect_servers}

    server {{
        listen 443 ssl;
        server_name {domain};
        ssl_certificate   /run/secrets/{domain}.crt;
        ssl_certificate_key   /run/secrets/{domain}.key;
        root /opt/web_root;
        {locations}
        location / {{
            # web app uri redirect
            try_files $uri $uri/ /index.html;
        }}
    }}
}}
    """.format(locations        = locations,
               upstreams        = upstreams,
               domain           = domain,
               redirect_servers = redirect_servers)

def get_nginx_proxy_conf_upstreams(config, platform_name):
    platform = config['platforms'][platform_name]
    # upstream directive for each port exposed to the internal network
    # ports: ["{name}:{protocol}:{port}"] |
    #        ["{name}:{port}"] |
    #        ["{port}"]
    #   name        - (optional) specify name to append to service name
    #   prototcol   - (optional) specify ws or http or something else
    #   port        - (required) port number
    upstreams = ''
    port_specs = get_platform_port_specs(config, platform_name)

    for port_name, protocol, port in port_specs:
        service_name = get_service_name(config, platform_name)

        if protocol == 'ws':
            upstreams += """
    upstream docker-{service_name} {{
        server {service_name}:{port};
    }}""".format(service_name  = service_name,
                 port          = port)
        else:
            raise ValueError("Invalid protocol")

    return upstreams

def get_nginx_proxy_conf_locations(config, platform_name):
    platform = config['platforms'][platform_name]
    # location directive for each port exposed to the internal network
    # ports: ["{name}:{protocol}:{port}"] |
    #        ["{name}:{port}"] |
    #        ["{port}"]
    #   name        - (optional) specify name to append to service name
    #   prototcol   - (optional) specify ws or http or something else
    #   port        - (required) port number
    # uri should be uri prefix plus service_name
    locations = ''
    port_specs = get_platform_port_specs(config, platform_name)

    for port_name, protocol, port in port_specs:
        service_name = get_service_name(config, platform_name)

        uri = config.get('uri_prefix') or '/'
        uri += service_name
        if len(port_name) > 0:
            uri += '_' + port_name
        if protocol == 'ws':
            locations += get_nginx_location_websocket(uri, service_name, port)
        else:
            raise ValueError("Invalid protocol")

    return locations

# returns [[name, protocol, port]]
def get_platform_port_specs(config, platform_name):
    platform = config['platforms'][platform_name]
    port_specs = platform.get('ports') or []
    results = []
    for port_spec in port_specs:
        port_data    = port_spec.split(':')
        length       = len(port_data)

        if length is 1:
            [port] = port_data
            port_name = ''
            protocol = 'ws'
        elif length is 2:
            [port_name, port] = port_data
            protocol = 'ws'
        elif length is 3:
            port_name, protocol, port = port_data
        else:
            raise ValueError("Invalid port specification")

        # only websockets are supported
        if protocol not in ['ws']:
            raise ValueError("Invalid protocol specified")

        results.append([port_name, protocol, port])

    return results

def get_nginx_location_websocket(uri, service_name, port):
    # creates a location with an exact uri match of the platform name
    # it proxies to the websocket backend
    return """
      location = {uri} {{
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Host $http_host;
        proxy_set_header X-NginX-Proxy true;

        proxy_pass http://docker-{service_name};
        proxy_redirect off;

        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
      }}""".format(uri           = uri,
                   service_name  = service_name,
                   port          = port)

def get_nginx_proxy_service(config, domain):
    service_name_fn = lambda x : get_service_name(config, x)
    service_names = map(service_name_fn, get_all_platforms(config))
    tag_name = get_image_tag_name(config, 'nginx_proxy', 'service')
    return {
        'nginx_proxy': {
            'image': tag_name,
            'ports': ['80:80', '443:443'],
            'volumes': [
                'web_root:/opt/web_root:ro'
            ],
            'secrets': [
                "{}.crt".format(domain),
                "{}.key".format(domain)
            ],
            'depends_on': service_names
        }
    }

def is_cluster_mode():
    # if DOCKER_HOST env var is set and we find at least one docker-machine node
    return len(cluster_get_build_node_ids()) > 0

def get_service_name(config, platform_name):
    return platform_name

def check_secret(name):
    p = subprocess.Popen(['docker', 'secret', 'inspect', name])
    p.communicate('')
    return p.returncode != 0

def generate_self_signed_ssl_certificate(domain):
    subj = '/C=US/ST=Nevada/L=Henderson/O=Dis/CN={}'.format(domain)
    try:
	key_data = subprocess.check_output(['openssl', 'genrsa', '4096'])
    except:
	raise RuntimeError('Failed to generate SSL key (via openssl)')

    # Not sure about this if we need it to ever be secure
    print "Creating self signed certificate..."
    p = subprocess.Popen([
        'openssl', 'req',
            '-new', '-x509', '-key', '/dev/stdin',
            '-subj', subj],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE)
    crt_data, crt_err = p.communicate(key_data)

    if p.returncode != 0:
	raise RuntimeError('Failed to generate SSL cert (via openssl)')

    return (domain, key_data, crt_data)

def import_ssl_certificate(domain, key_data, crt_data):
    # Add as docker secrets
    key_name = '{}.key'.format(domain)
    crt_name = '{}.crt'.format(domain)
    print 'Adding certificates to secret database...'
    p = subprocess.Popen(['docker', 'secret', 'create', key_name, '-'],
                         stdin=subprocess.PIPE)
                         #stdout=subprocess.PIPE)
    p.communicate(key_data)
    if p.returncode != 0:
	raise RuntimeError('Failed to create docker secret for {}'.format(key_name))

    p = subprocess.Popen(['docker', 'secret', 'create', crt_name, '-'],
                         stdin=subprocess.PIPE)
    p.communicate(crt_data)
    if p.returncode != 0:
	raise RuntimeError('Failed to create docker secret for {}'.format(crt_name))

def get_secret_files():
    # We never want to look in the source_path for secrets
    # so this is a valid use of getcwd
    secrets_dir = os.path.join(os.getcwd(), 'cppdock_secrets')
    if not os.path.isdir(secrets_dir):
        return {}
    names = [f for f in os.listdir(secrets_dir)]
    secrets = {}
    for name in names:
        path = os.path.abspath(os.path.join(secrets_dir, name))
        if os.path.isfile(path):
            secrets[name] = { 'file': path }

    return secrets

def get_registry(config):
    registry = config['cppdock'].get('registry')
    # TODO resolve "docker-machine:{name}"
    return registry or '127.0.0.1:5000'
    

dispatch(os.sys.argv[1:2], os.sys.argv[2:])
