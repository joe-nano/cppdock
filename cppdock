#!/usr/bin/env python

from collections import OrderedDict
from uuid import getnode as get_mac
import argparse
import glob
import json
import os
import string
import subprocess
import sys
import urllib2
import warnings
import zipfile
import distutils.dir_util

root_path = os.path.abspath('.')
default_json_file_path = root_path + '/cppdock.json'
default_dep_path = "https://github.com/"
target_registry = os.getenv('CPPDOCK_TARGET_REGISTRY') or None;
dev_registry = os.getenv('CPPDOCK_DEV_REGISTRY') or '127.0.0.1:5000';
cluster_build_nodes = None

def get_commands():
    return {
        'init':                 (command_init,              parse_args_init),
        'build':                (command_build,             parse_args_build),
        'build_nginx_proxy':    (command_build_nginx_proxy, parse_args_build_nginx_proxy),
        'dev':                  (command_dev,               parse_args_dev),
        'dev_cluster':          (command_dev_cluster,       parse_args_dev_cluster),
        'dev_registry':         (command_dev_registry,      parse_args_dev_registry),
        'install_dep':          (command_install_dep,       parse_args_install_dep),
        'install_src':          (command_install_src,       parse_args_install_src),
        'init_dev_service':     (command_init_dev_service,  parse_args_init_dev_service),
        '_complete':            (command_complete,          lambda _ : os.sys.argv[2]),
    }

def get_subcommands():
    return filter(lambda x: not x.startswith('_'), get_commands().keys())

def parse_args_command(args):
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument('command', choices = get_commands().keys())
    return parser.parse_args(args)

def parse_args_init(args):
    parser = argparse.ArgumentParser(prog='cppdock init')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')

    return parser.parse_args(args)

def parse_args_build(args):
    parser = argparse.ArgumentParser(prog='cppdock build')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')

    parser.add_argument('--print-only', dest      = 'print_only',
                                        action    = 'store_true',
                                        help      = 'Print Dockerfile without building')
    parser.add_argument('platform',     default   = None,
                                        nargs     = '?',
                                        help      = 'Name of target platform')

    return parser.parse_args(args)

def parse_args_build_nginx_proxy(args):
    parser = argparse.ArgumentParser(prog='cppdock build')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')

    parser.add_argument('--print-only', dest      = 'print_only',
                                        action    = 'store_true',
                                        help      = 'Print Dockerfile without building')

    return parser.parse_args(args)

def parse_args_dev(args):
    parser = argparse.ArgumentParser(prog='cppdock dev')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')
    parser.add_argument('-p', '--port', dest      = 'port',
                                        help      = 'Specify port to `docker run`')

    group  = parser.add_argument_group('required arguments')
    group.add_argument('platform',      help      = 'Name of target platform')

    return parser.parse_args(args)

def parse_args_dev_cluster(args):
    parser = argparse.ArgumentParser(prog='cppdock dev_cluster')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')
    parser.add_argument('--print-only', dest      = 'print_only',
                                        action    = 'store_true',
                                        help      = 'Print Composefil without deploying')
    parser.add_argument('--stop',       dest      = 'stop',
                                        action    = 'store_true',
                                        help      = 'Stops the dev cluster services')
    parser.add_argument('platform',     default   = None,
                                        nargs     = '?',
                                        help      = 'Name of service to shell into')

    return parser.parse_args(args)

def parse_args_dev_registry(args):
    parser = argparse.ArgumentParser(prog='cppdock dev_registry')
    parser.add_argument('--stop',       dest      = 'stop',
                                        action    = 'store_true',
                                        help      = 'Stops the dev registry')
    return parser.parse_args(args)

def parse_args_install_dep(args):
    parser = argparse.ArgumentParser(prog='cppdock install_dep')
    group  = parser.add_argument_group('required arguments')
    group.add_argument('platform',      help      = 'Token for target platform')
    group.add_argument('branch',        help      = 'SHA1 of git revision')
    group.add_argument('repo',          help      = 'Name of git repo (e.g. boostorg/hana)')
    return parser.parse_args(args)

def parse_args_install_src(args):
    parser = argparse.ArgumentParser(prog='cppdock install_src')
    group  = parser.add_argument_group('required arguments')
    group.add_argument('branch',        help      = 'SHA1 of git revision')
    group.add_argument('repo',          help      = 'Name of git repo (e.g. llvm-mirror/clang)')
    group.add_argument('dest_dir',      help      = 'Path to copy source files to')
    group  = parser.add_argument_group('optional arguments')
    group.add_argument('src_dir',       nargs='?',
                                        default='.',
                                        help      = 'Path of subdirectory to copy from repo')
    return parser.parse_args(args)

def parse_args_init_dev_service(args):
    parser = argparse.ArgumentParser(prog='cppdock init_dev_service')
    parser.add_argument('--shell-only', dest      = 'shell_only',
                                        action    = 'store_true',
                                        help      = 'Run only as an interactive shell')
    group  = parser.add_argument_group('required arguments')
    group.add_argument('platform',      help      = 'Token for target platform')

    #TODO Add a --shell-only option to be called by `cppdock dev ...`
    assert not args.shell_only, "--shell-only option not implemented"

    return parser.parse_args(args)

def dispatch(command_args, cli_args):
    command = parse_args_command(command_args).command
    commands = get_commands()
    if command in commands:
        args = commands[command][1](cli_args)
        commands[command][0](args)

def command_complete(args_str):
    subcommands = get_subcommands()

    args = args_str.split();
    has_trailing_space = args_str.endswith(' ')
    # length includes a trailing empty string if there is a trailing space
    if args_str.endswith(' '):
        args.append('')
    # filter out completed options
    args = filter(lambda x: not x.startswith('-'), args[:-1]) + [args[-1]]
    length = len(args)

    if length < 2:
        # should not be possible from the completion script
        return

    candidates = []
    completion = ''

    if length == 2:
        # complete subcommand
        completion = args[1]
        candidates = subcommands
    elif length == 3:
        # complete platform
        subcommand = args[1]
        if subcommand in ['build', 'dev', 'dev_cluster']:
            completion = args[2]
            config = load_config_file(default_json_file_path)
            candidates = get_all_platforms(config)

    candidates = filter(lambda x: x.startswith(completion), candidates)
    print ' '.join(candidates)

def command_init(args):
    load_config_file_with_args(args)

def command_build(args):
    config = load_config_file_with_args(args)

    platforms = [args.platform] if args.platform else get_all_platforms(config)
    for platform in platforms:
        make_build_image(config, platform, args.print_only)

    if not args.platform and requires_nginx_proxy(config):
        make_nginx_proxy_image(config, args.print_only)

def make_build_image(config, platform, print_only):
    dockerfile = make_deps_dockerfile(config, platform)
    tag_name = get_build_tag_name(config, platform)
    if print_only:
        print '\n\n\n###### Docker Image: ' + tag_name
        print dockerfile
    else:
        make_base_image_dockerfile(config, platform)
        make_dep_image(config, platform, dockerfile)

def command_build_nginx_proxy(args):
    config = load_config_file_with_args(args)
    make_nginx_proxy_image(config, args.print_only)

def make_nginx_proxy_image(config, print_only):
    version = 'latest'
    tag_name = get_image_tag_name(config, 'nginx_proxy', 'service', version)
    conf_data = get_nginx_proxy_conf(config)
    dockerfile = """
    FROM nginx:alpine
        RUN echo $'{conf_data}' > /etc/nginx/nginx.conf
    """.format(conf_data = conf_data.replace('\n', '\\n\\\n'))

    if (print_only):
        print '\n\n\n###### Docker Image: ' + tag_name
        print dockerfile
    else:
        make_docker_image(tag_name, dockerfile)

def command_dev(args):
    config = load_config_file_with_args(args)
    platform_name = args.platform
    ports_arg = ' -p=' + args.port if hasattr(args, 'port') and args.port else ''
    make_dev_image(config, platform_name)
    run_dev_image(config, platform_name, ports_arg)

def command_dev_cluster(args):
    config = load_config_file_with_args(args)
    stack_name = get_stack_name(config, 'dev')
    version = 'latest'

    if (args.stop):
        subprocess.check_output(['docker', 'stack', 'rm', stack_name])
    elif (args.print_only):
        print get_dev_compose_file(config, stack_name, version)
    else:
        nodes = cluster_get_build_node_ids()
        if is_cluster_mode():
            print "Running in CLUSTER mode...\n"
            print "Nodes found: " + ', '.join(nodes)
        else:
            print "Running in LOCAL mode..."

        cluster_sync_source(nodes)
        # builds and pushes the dev images which should be to the dev_registry
        for platform_name in get_all_platforms(config):
            tag_name = get_dev_tag_name(config, platform_name)
            make_dev_image(config, platform_name)
            print "Pushing dev image: {}".format(tag_name)
            subprocess.check_output(['docker', 'push', tag_name])

        run_stack_deploy(config, stack_name, version)
        run_stack_service_terminal(config, stack_name, args.platform)

        dev_service_name = subprocess.check_output(['docker', 'ps', '-q', '-f name=servicename'])
        # if args.platform:
        # TODO docker exec -it ${dev_service_platform_name}

def command_dev_registry(args):
    # docker service create --name test_registry --publish published=5000,target=5000 registry
    env_val = os.getenv('CPPDOCK_DEV_REGISTRY')
    registry_name = 'cppdock-dev-registry'
    if env_val:
        raise 'ERROR: CPPDOCK_DEV_REGISTRY is set ({})'.format(env_val)

    if args.stop:
        subprocess.check_output(['docker', 'service', 'rm', registry_name])
    else:
        subprocess.check_output(['docker', 'service', 'create', '--name',
                                 registry_name, '--publish', '5000:5000', 'registry'])


def command_install_dep(args):
    check_sysroot()
    url = make_archive_url(args.repo, args.branch)
    input = urllib2.urlopen(url)
    output = open('dep.zip', 'wr')
    buf = ''
    while True:
        buf = input.read(800)
        output.write(buf)
        output.flush()
        if (len(buf) == 0):
            break
    output.close()

    assert os.path.isfile('dep.zip')
    assert zipfile.is_zipfile('dep.zip')

    zipfile.ZipFile('dep.zip', 'r').extractall()
    recipe = find_recipe(args.platform, args.repo)
    cwd_dir = [x for x in os.listdir('.') if x.endswith(args.branch)][0]
    p = subprocess.Popen([recipe], cwd = cwd_dir)
    p.communicate('')
    if p.returncode != 0:
        print """

    FAILURE: Recipe returned error code {0}.
        """.format(p.returncode)
        sys.exit(1)

def command_install_src(args):
    url = make_archive_url(args.repo, args.branch)
    input = urllib2.urlopen(url)
    output = open('dep.zip', 'wr')
    buf = ''
    while True:
        buf = input.read(800)
        output.write(buf)
        output.flush()
        if (len(buf) == 0):
            break
    output.close()

    assert os.path.isfile('dep.zip')
    assert zipfile.is_zipfile('dep.zip')

    zipfile.ZipFile('dep.zip', 'r').extractall()
    base_src_dir = [x for x in os.listdir('.') if x.endswith(args.branch)][0]
    src_dir = os.path.join(base_src_dir, args.src_dir);
    distutils.dir_util.copy_tree(src_dir, args.dest_dir)
    distutils.dir_util.remove_tree(base_src_dir);

def command_init_dev_service(args):
    config = load_config_file_with_args(args)
    assert args.platform in get_all_platforms(config), "Invalid platform name"
    run_dev_service_target(config, args.platform)

def install_deps_platform(platform, items):
    for i in range(len(items)):
        repo, branch = items[i]
        branch = branch[0:40]
        install_dep(i, platform, repo, branch)

def load_config_file_with_args(args):
    json_file_path = args.json_file_path
    return load_config_file(json_file_path)

def load_config_file(json_file_path):
    if not os.path.isfile(json_file_path):
        raise ValueError('Config file not found: ' + json_file_path)

    stream = file(json_file_path, 'r')
    config = json.load(stream, object_pairs_hook=OrderedDict)

    for sections in config:
        if sections == 'cppdock':
            continue
        for platform, settings in config[sections].items():
            check_base_image_conflict(settings)

            for stage in settings['deps']:
                if not isinstance(stage, list):
                    raise "Each stage in 'deps' must be an array."
                for item in stage:
                    item = lock_item_json(item)

    write_config_file(config, json_file_path)
    return config

def write_config_file(config, json_file_path):
    try:
        with open(json_file_path, 'wb') as fh:
            json.dump(config, fh, indent=2, separators=(',', ': '))
            fh.write('\n')
    except:
        warnings.warn('WARNING: Unable to write to json file')

def lock_item_json(item):
    current_sha = get_current_sha_with_comment(item)
    item.update(revision = current_sha)

    return item

def get_current_sha_with_comment(item):
    repo = item['name']
    tag = item.get('tag')
    source = item.get('source')

    if 'revision' in item:
        return item['revision']
    else:
        if source == 'docker':
            tag = tag or 'latest'
            repo_tag = repo + ":" + tag

            try:
                # pull the image from the registry to get the RepoDigest
                subprocess.check_output(['docker', 'pull', repo_tag])
            except:
                raise RuntimeError("Unable to pull docker image from registry: " + repo_tag )

            try:
                inspect_result =  subprocess.check_output(['docker', 'inspect', repo_tag])
                digest = json.loads(inspect_result)[0]['RepoDigests'][0].split('@')[1]
            except:
                raise RuntimeError("Unable to get SHA from docker image")

            return digest
        else:
            tag = normalize_branch_name(tag)
            lines = subprocess.check_output(['git', 'ls-remote', normalize_repo_name(repo)]).splitlines()
            for line in lines:
                if line.endswith(tag):
                    return line[0:40]
            raise RuntimeError("Unable to get SHA from remote git repository")

def normalize_repo_name(name):
    return default_dep_path + name;

def normalize_branch_name(name):
    if name == None or len(name) == 0 or name == 'HEAD':
        return 'HEAD'
    elif name.startswith('refs/'):
        return name
    else:
        return "refs/heads/" + name

def is_sha(name):
    return len(name) == 40

def check_base_image_conflict(settings):
    if 'base_image' in settings and 'base_image_dockerfile' in settings:
        raise StandardError("Conflicting base image settings detected")

def get_max_key_length(config):
    length = 0
    for section in config.sections():
        for item in config.items(section):
            len_ = len(item[0])
            if (len_ > length):
                length = len_
    return length

def make_archive_url(repo, branch):
    return 'https://github.com/{0}/archive/{1}.zip'.format(repo, branch)

def get_base_image_dockerfile(config, platform_name):
    return config['platforms'][platform_name].get('base_image_dockerfile')

def make_base_image_dockerfile(config, platform_name):
    base_image_dockerfile = get_base_image_dockerfile(config, platform_name)

    if base_image_dockerfile:
        dockerfile = "./{0}".format(base_image_dockerfile)
        tag_name = "{0}_platform:latest".format(get_project_name(config))

        p = subprocess.Popen(['docker', 'build', '--tag=' + tag_name, '--file='+ dockerfile, '.'], stdin=subprocess.PIPE)
        out, err = p.communicate('')

        if p.returncode != 0:
            print """
    FAILURE
            """

def find_recipe(platform, repo):
    repo = repo.replace('/', '-')
    local_recipe_path = '/opt/cppdock_recipes'
    repo_with_platform = repo + '-' + platform
    builtin_recipe_path = os.path.expanduser('~') + '/.cppdock_recipes'
    xs = [
        local_recipe_path   + '/' + repo_with_platform,
        local_recipe_path   + '/' + repo,
        local_recipe_path   + '/default',
        builtin_recipe_path + '/' + repo_with_platform,
        builtin_recipe_path + '/' + repo,
        builtin_recipe_path + '/default',
    ]
    for x in xs:
        print "Trying Recipe Location: {0}".format(x)
        if os.path.isfile(x):
            return x
    raise ValueError('Unable to find cppdock recipe: ' + repo_with_platform)

def check_sysroot():
    if not os.path.isdir('/opt/sysroot/'):
        raise ValueError('Sysroot directory is missing: /opt/sysroot/')

def get_arg(args, i):
    next(iter(args[i:i+1]), None)

def get_config_option(config, name):
    defaults = {
        'project': None,
        'platform_linux_x64':     'ricejasonf/cppdock:linux_x64',
        'platform_emscripten':    'ricejasonf/cppdock:emscripten',
        'platform_tvossimulator': 'ricejasonf/cppdock:tvossimulator',
    }
    value = None

    if name in defaults:
        value = defaults[name]
    if value == None:
        raise ValueError('Config option has no default for "{0}"'.format(name))
    return value

def make_dep_image(config, platform_name, dockerfile):
    tag_name = get_build_tag_name(config, platform_name)
    (returncode, out, err) = make_docker_image(tag_name, dockerfile)
    if returncode == 0:
        print "Finished building {0}.".format(tag_name)
    else:
        print """

    FAILURE: Build of {0} FAILED.

        """.format(tag_name)
        sys.exit(1)

def make_docker_image(tag_name, dockerfile_data):
    p = subprocess.Popen(['docker', 'build', '--tag=' + tag_name, '--file=-', '.'], stdin=subprocess.PIPE)
    out, err = p.communicate(dockerfile_data)
    return (p.returncode, out, err)

def get_platform_type(config, platform_name):
    try:
        return config['platforms'][platform_name]['type']
    except:
        raise ValueError('Platform name `{0}` not found.'.format(platform_name))

def get_project_name(config):
    return config['cppdock']['name']

def make_deps_dockerfile(config, platform_name):
    platform_type = get_platform_type(config, platform_name)
    cppdock_image = get_base_image(config, platform_name) or get_config_option(config, 'platform_' + platform_type)
    deps = get_deps_from_config(config, platform_name)
    deps_stages = ''
    deps_imports = ''

    for dep in deps:
        deps_stages += string.join([make_deps_stage(cppdock_image, platform_name, dep)])
        deps_imports += make_deps_import(platform_name, dep[-1][2])

    return """
{0}
FROM {1}
{2}
""".format(deps_stages, cppdock_image, deps_imports)

def get_base_image(config, platform_name):
    if 'base_image_dockerfile' in config['platforms'][platform_name]:
        return '{0}_platform'.format(get_project_name(config))
    else:
        return config['platforms'][platform_name].get('base_image')

# deprecated
def make_deps_dockerfile_stageless(config, platform):
    cppdock_image  = get_config_option(config, 'platform_' + platform)
    deps = get_deps_from_config(config, platform)
    deps_stages = string.join([make_install_dep(x, y, z, zz) for x, y, z, zz in deps])

    return """
FROM {0} as build_all
{1}

FROM {2}

    COPY --from=build_all /opt/install /opt/sysroot
""".format(cppdock_image, deps_stages, cppdock_image)

def make_deps_stage(cppdock_image, platform_name, deps):
    copy_recipes_term = ""
    build_image = cppdock_image

    # only one dep that is source: docker does not use make_install_dep

    docker_image_digest = get_stage_docker_image_digest(deps) 
    if docker_image_digest is not None:
    	build_image  = docker_image_digest
        install_deps = ''
    else:
        install_deps = string.join([make_install_dep(x, y, z, zz) for x, y, z, zz in deps])

    revision = deps[-1][2].replace('/', '_')

    return """
FROM {0} as build_{2}_{1}
    {4}
    WORKDIR /usr/local/src
    {3}
""".format(build_image, platform_name, revision, install_deps, copy_recipes_term)

def get_stage_docker_image_digest(stage_deps):
    if len(stage_deps) == 1:
        for platform, revision, repo, source in stage_deps:
            if source == 'docker':
                # repo@revision
                return '{0}@{1}'.format(repo, revision)
    return None


def make_install_dep(platform, revision, repo, source=''):
    if source == 'docker':
        return """
    COPY --from={0}@{1} /opt/install /opt/sysroot
    COPY --from={0}@{1} /opt/install /opt/install""".format(repo, revision)

    if not is_sha(revision):
        return ''
    repo_recipe_prefix = repo.replace('/', '-')
    copy_local_recipe = ''
    if os.path.isfile(os.path.join(root_path, 'cppdock_recipes', repo_recipe_prefix + '-' + platform)):
        copy_local_recipe = """
    COPY cppdock_recipes/{0}-{1} /opt/cppdock_recipes/""".format(repo_recipe_prefix, platform)
    elif os.path.isfile(os.path.join(root_path, 'cppdock_recipes', repo_recipe_prefix)):
        copy_local_recipe = """
    COPY cppdock_recipes/{0} /opt/cppdock_recipes/""".format(repo_recipe_prefix)

    return """{3}
    RUN cppdock install_dep {0} {1} {2}""".format(platform, revision, repo, copy_local_recipe)

def make_deps_import(platform, revision):
    return """
    COPY --from=build_{1}_{0} /opt/install/ /opt/sysroot""".format(platform, revision.replace('/', '_'))

def get_deps_from_config(config, platform_name):
    platform = config['platforms'][platform_name]

    return [get_deps_stage_items(stage, platform['type']) for stage in platform['deps']]

    raise ValueError('Platform is not specified in cppdock json file')

def get_deps_stage_items(deps_stage, platform_type):
    return [(platform_type, item['revision'],  item['name'], item.get('source')) for item in deps_stage]

def make_dev_image(config, platform_name):
    build_tag_name = get_build_tag_name(config, platform_name)
    dev_tag_name = get_dev_tag_name(config, platform_name)
    build_type = os.environ.get('BUILD_TYPE', None) or default_build_type(config, platform_name)
    dockerfile = """
FROM {0}
    WORKDIR /opt/build
    CMD cmake \
        -DCMAKE_BUILD_TYPE={1} \
        -DCMAKE_TOOLCHAIN_FILE='/opt/toolchain.cmake' \
        /opt/src \
        && /bin/bash
""".format(build_tag_name, build_type)

    p = subprocess.Popen(['docker', 'build', '--tag=' + dev_tag_name, '--file=-', '.'], stdin=subprocess.PIPE)
    out, err = p.communicate(dockerfile)
    if p.returncode == 0:
        print "Finished building {0}.".format(dev_tag_name)
    else:
        print """

    FAILURE: Build of {0} FAILED.
        """.format(dev_tag_name)
        sys.exit(1)

def run_dev_image(config, platform_name, ports_arg):
    tag_name = get_dev_tag_name(config, platform_name)
    pwd = os.getcwd()
    os.system('docker run --rm -it ' + ports_arg + ' --mount type=bind,source=' + pwd + ',target=/opt/src,readonly ' + tag_name)

def run_stack_deploy(config, stack_name, version='latest'):
    # docker stack deploy is already idempotent
    compose_file_data = get_dev_compose_file(config, stack_name, version)
    p = subprocess.Popen(['docker', 'stack', 'deploy', '--prune', '--compose-file', '-', stack_name], stdin=subprocess.PIPE)
    p.communicate(input=compose_file_data)
    if p.returncode != 0:
        print """

    FAILURE: `docker stack deploy` for `{stack_name}` error code {error_code}.
        """.format(stack_name=stack_name, error_code=p.returncode)
        sys.exit(1)

def run_stack_service_terminal(config, stack_name, platform_name):
    print """
    TODO shell into {stack_name}.{platform_name}
    """
    # container_id = # get_container_id(stack_name, platform_name)
    # TODO finish
    #os.system("docker exec -it {0} bash".format(container_id)

def get_dev_compose_file(config, stack_name, version):
    platform_names = get_all_platforms(config)
    services = [get_dev_compose_file_service(config, stack_name, platform_name, version) for platform_name in platform_names]

    services += get_nginx_proxy_service(config)
    services_s = ''.join(services)
    result = """
    version: '3'
    services: {0}
    volumes:
        web_root:
    """.format(services_s)
    return result

def get_dev_compose_file_service(config, stack_name, platform_name, version='latest'):
    platform = get_platform_config(config, platform_name)
    image_tag = get_image_tag_name(config, platform_name, 'dev', version)
    dev_target = platform.get('dev_target')
    command = "make {}".format(dev_target) if dev_target else "tail -f /dev/null"
    command = """ "
              cmake \\
                -DCMAKE_BUILD_TYPE=Debug \\
                -DCMAKE_TOOLCHAIN_FILE='/opt/toolchain.cmake' \\
                /opt/src \\
              && {command} "
    """.format(command = command)

    web_root = platform.get('web_root')
    if web_root:
        web_root = "- web_root:{0}".format(web_root)

    pwd = os.getcwd()
    remote_pwd = cluster_get_sync_path(pwd, 'src')
    src_mount_path = remote_pwd if is_cluster_mode() else pwd

    return """
        {platform}:
            image: {image}
            command: {command}
            volumes:
                - {src_mount_path}:/opt/src:ro
                - /opt/build
                {web_root}
    """.format(platform       = platform_name,
               stack_name     = stack_name,
               image          = image_tag,
               command        = command,
               web_root       = web_root or '',
               src_mount_path = src_mount_path)

def get_build_tag_name(config, platform_name, version="latest"):
    return get_image_tag_name(config, platform_name, "build", version)

def get_dev_tag_name(config, platform_name, version="latest"):
    return get_image_tag_name(config, platform_name, "dev", version)

def get_image_tag_name(config, platform_name, image_type, version):
    registry = get_registry(config, image_type)
    assert(image_type in ['build', 'dev', 'release', 'service'])
    return "{registry}{project}-{platform}{image_type}:{version}".format(
        registry    = registry + '/' if registry else '',
        project     = get_project_name(config),
        platform    = platform_name,
        image_type  = '-' + image_type if image_type != 'release' else '',
        version     = version,
    )

def get_stack_name(config, mode):
    assert(mode in ['dev', 'release'])
    name = get_project_name(config) + '-stack'

    if mode == 'dev':
        name += '-dev'
    return name

def get_project_name(config):
    try:
        return config['cppdock']['name']
    except:
        raise ValueError('Project name not found in cppdock.json.')

def get_registry(config, image_type):
    assert(image_type in ['build', 'dev', 'release', 'service'])
    if image_type == 'build':
        return None
    elif image_type == 'dev':
        return dev_registry
    else:
        return target_registry

def default_build_type(config, platform_name):
    if config['platforms'][platform_name]['type'] == 'emscripten' :
        return 'Release'
    else:
        return 'Debug'

def get_all_platforms(config):
    return config['platforms'].keys()

def get_platform_config(config, platform_name):
    # TODO maybe merge overrides for dev or something
    return config['platforms'][platform_name]

def run_dev_service_target_init(config):
    p = subprocess.Popen(['cmake', '-DCMAKE_BUILD_TYPE=Debug', 
                          "-DCMAKE_TOOLCHAIN_FILE='/opt/toolchain.cmake",
                          '/opt/src'],
                         cwd = '/opt/build')
    p.communicate('')
    return p.returncode == 0

def run_dev_service_target(config, platform):
    # This should run in a docker container
    # Initializes cmake if needed

    p = None
    if not os.path.isfile('/opt/build/CMakeCache.txt') and run_dev_service_target_init(config):
        p = subprocess.Popen(['cmake', '--build', '.',
                              '--target',
                              "cppdock_dev_service.{}".format(platform)],
                             cwd = '/opt/build')
        p.communicate('')

    # Make the cppdock_dev_service target or just hang out
    # Things will show up in the logs and the user can still shell in to inspect things


    # if it fails or doesn't run then just hang out so the container stays alive

    if not (p and p.returncode == 0):
        p = subprocess.Popen(['tail', '-f', '/dev/null'])
        p.communicate('')

def cluster_get_worker_token():
    try:
        return subprocess.check_output(['docker', 'swarm', 'join-token', '-q', 'worker'])
    except:
        raise RuntimeError("Unable to get cluster worker token")

def cluster_get_build_node_ids():
    global cluster_build_nodes
    if cluster_build_nodes is not None:
        return cluster_build_nodes
    try:
        result = subprocess.check_output(['docker-machine', 'ls',
                                          '-q', '--filter', 'label=cppdock_build=1',
                                          '--filter', 'state=Running'])
        cluster_build_nodes = result.splitlines()
        return cluster_build_nodes
    except:
        raise RuntimeError("Unable to get cluster ids for cppdock_build nodes")

def cluster_get_sync_path(path, kind):
    assert kind in ['src', 'secret']
    return "/home/ubuntu/sync{path}/{kind}".format(path = path,
                                                   kind = kind)

# rsyncs everything in pwd to nodes in cluster
def cluster_sync_source(nodes):
    nodes = cluster_get_build_node_ids()
    pwd = os.getcwd()
    for node in nodes:
        path = cluster_get_sync_path(pwd, 'src')
        p = subprocess.Popen(['docker-machine', 'ssh', node,
                              'mkdir', '-p' ' {path}'.format(path=path)])
        p.communicate('')
        if p.returncode != 0:
            raise RuntimeError("Unable to make the sync directory on {node}".format(node = node))
        node_path = "ubuntu@{node}:{path}".format(node = node,
                                           path = path)
        p = subprocess.Popen(['docker-machine', 'scp', '-r', '-d', '.', node_path])
        p.communicate('')

        if p.returncode != 0:
            raise RuntimeError("Failed to sync source files to {node}".format(node = node))

def get_nginx_proxy_conf(config):
    locations = ''
    upstreams = ''
    for platform_name in get_all_platforms(config):
        locations += get_nginx_proxy_conf_locations(config, platform_name)
        upstreams += get_nginx_proxy_conf_upstreams(config, platform_name)
    return """
user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {{
  worker_connections  1024;
}}

http {{
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
    resolver 127.0.0.11;

    log_format  main  \\'$remote_addr - $remote_user [$time_local] "$request" \\'
		      \\'$status $body_bytes_sent "$http_referer" \\'
		      \\'"$http_user_agent" "$http_x_forwarded_for"\\';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    {upstreams}

    server {{
      listen 443 ssl;
      #server_name ip-address;
      ssl_certificate   /opt/certs/server.crt;
      ssl_certificate_key   /opt/certs/server.key;
      root /opt/web_root; {locations}
    }}
}}
    """.format(locations = locations,
               upstreams = upstreams)

def get_nginx_proxy_conf_upstreams(config, platform_name):
    platform = config['platforms'][platform_name]
    # upstream directive for each port exposed to the internal network
    # ports: ["{name}:{protocol}:{port}"] |
    #        ["{name}:{port}"] |
    #        ["{port}"]
    #   name        - (optional) specify name to append to service name
    #   prototcol   - (optional) specify ws or http or something else
    #   port        - (required) port number 
    upstreams = ''
    port_specs = get_platform_port_specs(config, platform_name)

    for port_name, protocol, port in port_specs:
        service_name = platform_name

        if protocol == 'ws':
            upstreams += """
    upstream docker-{platform_name} {{
        server {platform_name}:{port};
    }}""".format(platform_name = platform_name,
                 port          = port)
        else:
            raise ValueError("Invalid protocol")

    return upstreams

def get_nginx_proxy_conf_locations(config, platform_name):
    platform = config['platforms'][platform_name]
    # location directive for each port exposed to the internal network
    # ports: ["{name}:{protocol}:{port}"] |
    #        ["{name}:{port}"] |
    #        ["{port}"]
    #   name        - (optional) specify name to append to service name
    #   prototcol   - (optional) specify ws or http or something else
    #   port        - (required) port number 
    # uri should be uri prefix plus service_name
    locations = ''
    port_specs = get_platform_port_specs(config, platform_name)

    for port_name, protocol, port in port_specs:
        service_name = platform_name

        uri = config.get('uri_prefix') or '/'
        uri += service_name
        if len(port_name) > 0:
            uri += '_' + port_name
        if protocol == 'ws':
            locations += get_nginx_location_websocket(uri, platform_name, port)
        else:
            raise ValueError("Invalid protocol")

    return locations

# returns [[name, protocol, port]]
def get_platform_port_specs(config, platform_name):
    platform = config['platforms'][platform_name]
    port_specs = platform.get('ports') or []
    results = []
    for port_spec in port_specs:
        port_data    = port_spec.split(':')
        length       = len(port_data)

        if length is 1:
            [port] = port_data
            port_name = ''
            protocol = 'ws'
        elif length is 2:
            [port_name, port] = port_data
            protocol = 'ws'
        elif length is 3:
            port_name, protocol, port = port_data
        else:
            raise ValueError("Invalid port specification")

        # only websockets are supported
        if protocol not in ['ws']:
            raise ValueError("Invalid protocol specified")

        results.append([port_name, protocol, port])

    return results

def get_nginx_location_websocket(uri, platform_name, port):
    return """
      location {uri} {{
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Host $http_host;
        proxy_set_header X-NginX-Proxy true;

        proxy_pass http://docker-{platform_name};
        proxy_redirect off;

        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
      }}""".format(uri           = uri,
                   platform_name = platform_name,
                   port          = port)

def get_nginx_proxy_service(config):
    version = 'latest'
    service_names = ', '.join(get_all_platforms(config))
    tag_name = get_image_tag_name(config, 'nginx_proxy', 'service', version)
    return """
        nginx_proxy:
            image: {tag_name} 
            volumes:
                - web_root:/opt/web_root:ro
            depends_on: [{service_names}]
    """.format(tag_name      = tag_name,
               service_names = service_names)

def requires_nginx_proxy(config):
    # We could check a configure property
    # if we want to allow an opt-out
    return True;

def is_cluster_mode():
    # if DOCKER_HOST env var is set and we find at least one docker-machine node
    return len(cluster_get_build_node_ids()) > 0


dispatch(os.sys.argv[1:2], os.sys.argv[2:])
